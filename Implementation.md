# Telegram AI Bot on Convex â€” Implementation Guide

## Architecture Overview

```
Telegram Group
    â”‚
    â”‚ (webhook POST)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Convex HTTP Action             â”‚
â”‚  POST /api/telegram-webhook     â”‚
â”‚  - Validate request             â”‚
â”‚  - Parse update                 â”‚
â”‚  - Store user message           â”‚
â”‚  - Schedule AI processing       â”‚
â”‚  - Return 200 immediately       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ ctx.scheduler.runAfter(0, ...)
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Convex Action: processMessage  â”‚
â”‚  - Query recent conversation    â”‚
â”‚  - Build prompt with context    â”‚
â”‚  - Call Claude / OpenAI API     â”‚
â”‚  - Store assistant response     â”‚
â”‚  - POST sendMessage to Telegram â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The key design choice is **async processing**: the webhook handler returns `200 OK` to Telegram immediately, then schedules the AI work in a separate action. This avoids Telegram retry storms and keeps the webhook fast.

---

## Project Structure

```
telegram-ai-bot/
â”œâ”€â”€ convex/
â”‚   â”œâ”€â”€ _generated/          # Auto-generated by Convex
â”‚   â”œâ”€â”€ schema.ts            # Database schema
â”‚   â”œâ”€â”€ http.ts              # HTTP webhook endpoint
â”‚   â”œâ”€â”€ telegram.ts          # Core bot logic (actions)
â”‚   â”œâ”€â”€ messages.ts          # Message storage (mutations + queries)
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ ai.ts            # AI provider abstraction
â”‚       â””â”€â”€ telegramApi.ts   # Telegram Bot API helpers
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ .env.local               # Local secrets (not committed)
```

---

## Step 1: Project Setup

### Initialize the project

```bash
npm create convex@latest telegram-ai-bot
cd telegram-ai-bot
npm install convex
```

### Set environment variables

In the Convex dashboard (or via CLI), set these environment variables:

| Variable                  | Description                                       |
| ------------------------- | ------------------------------------------------- |
| `TELEGRAM_BOT_TOKEN`      | From BotFather (`/newbot`)                        |
| `TELEGRAM_WEBHOOK_SECRET` | Random string you generate for webhook validation |
| `AI_PROVIDER`             | `"claude"` or `"openai"`                          |
| `AI_API_KEY`              | Your Claude or OpenAI API key                     |
| `AI_MODEL`                | e.g. `"claude-sonnet-4-20250514"` or `"gpt-4o"`   |
| `BOT_USERNAME`            | Your bot's username (without @)                   |

```bash
npx convex env set TELEGRAM_BOT_TOKEN "7123456789:AAF..."
npx convex env set TELEGRAM_WEBHOOK_SECRET "my-random-secret-string"
npx convex env set AI_PROVIDER "claude"
npx convex env set AI_API_KEY "sk-ant-..."
npx convex env set AI_MODEL "claude-sonnet-4-20250514"
npx convex env set BOT_USERNAME "my_ai_bot"
```

---

## Step 2: Database Schema

### `convex/schema.ts`

```typescript
import { defineSchema, defineTable } from "convex/server";
import { v } from "convex/values";

export default defineSchema({
  // Store every message (user + assistant) for context
  messages: defineTable({
    chatId: v.number(), // Telegram chat ID (group)
    userId: v.optional(v.number()), // Telegram user ID
    userName: v.optional(v.string()),
    role: v.union(v.literal("user"), v.literal("assistant")),
    text: v.string(),
    telegramMessageId: v.optional(v.number()),
    timestamp: v.number(), // Unix timestamp in ms
  })
    .index("by_chat", ["chatId", "timestamp"])
    .index("by_chat_recent", ["chatId"]),

  // Per-group configuration
  chats: defineTable({
    chatId: v.number(),
    chatTitle: v.optional(v.string()),
    systemPrompt: v.optional(v.string()),
    maxContextMessages: v.optional(v.number()),
    enabled: v.boolean(),
    createdAt: v.number(),
  }).index("by_chatId", ["chatId"]),

  // Simple rate limiting
  rateLimits: defineTable({
    chatId: v.number(),
    userId: v.number(),
    windowStart: v.number(), // Start of current window (ms)
    count: v.number(), // Requests in current window
  }).index("by_chat_user", ["chatId", "userId"]),
});
```

**Design notes:**

- Messages are indexed by `chatId + timestamp` so you can efficiently query the last N messages for context.
- The `chats` table lets each group customize the bot's personality via `systemPrompt`.
- Rate limits use a sliding window approach per user per group.

---

## Step 3: Message Storage Layer

### `convex/messages.ts`

```typescript
import { mutation, query } from "./_generated/server";
import { v } from "convex/values";

// Store an incoming user or assistant message
export const store = mutation({
  args: {
    chatId: v.number(),
    userId: v.optional(v.number()),
    userName: v.optional(v.string()),
    role: v.union(v.literal("user"), v.literal("assistant")),
    text: v.string(),
    telegramMessageId: v.optional(v.number()),
  },
  handler: async (ctx, args) => {
    return await ctx.db.insert("messages", {
      ...args,
      timestamp: Date.now(),
    });
  },
});

// Get recent messages for a chat (for AI context)
export const getRecent = query({
  args: {
    chatId: v.number(),
    limit: v.optional(v.number()),
  },
  handler: async (ctx, args) => {
    const limit = args.limit ?? 30;
    const messages = await ctx.db
      .query("messages")
      .withIndex("by_chat", (q) => q.eq("chatId", args.chatId))
      .order("desc")
      .take(limit);

    // Return in chronological order
    return messages.reverse();
  },
});

// Get or create chat config
export const ensureChat = mutation({
  args: {
    chatId: v.number(),
    chatTitle: v.optional(v.string()),
  },
  handler: async (ctx, args) => {
    const existing = await ctx.db
      .query("chats")
      .withIndex("by_chatId", (q) => q.eq("chatId", args.chatId))
      .unique();

    if (existing) {
      // Update title if changed
      if (args.chatTitle && args.chatTitle !== existing.chatTitle) {
        await ctx.db.patch(existing._id, { chatTitle: args.chatTitle });
      }
      return existing;
    }

    const id = await ctx.db.insert("chats", {
      chatId: args.chatId,
      chatTitle: args.chatTitle,
      enabled: true,
      createdAt: Date.now(),
    });
    return await ctx.db.get(id);
  },
});

// Get chat config
export const getChat = query({
  args: { chatId: v.number() },
  handler: async (ctx, args) => {
    return await ctx.db
      .query("chats")
      .withIndex("by_chatId", (q) => q.eq("chatId", args.chatId))
      .unique();
  },
});

// Check and update rate limit. Returns true if allowed.
export const checkRateLimit = mutation({
  args: {
    chatId: v.number(),
    userId: v.number(),
    maxPerMinute: v.optional(v.number()),
  },
  handler: async (ctx, args) => {
    const maxPerMinute = args.maxPerMinute ?? 10;
    const now = Date.now();
    const windowMs = 60_000;

    const existing = await ctx.db
      .query("rateLimits")
      .withIndex("by_chat_user", (q) =>
        q.eq("chatId", args.chatId).eq("userId", args.userId),
      )
      .unique();

    if (!existing) {
      await ctx.db.insert("rateLimits", {
        chatId: args.chatId,
        userId: args.userId,
        windowStart: now,
        count: 1,
      });
      return true;
    }

    // Reset window if expired
    if (now - existing.windowStart > windowMs) {
      await ctx.db.patch(existing._id, {
        windowStart: now,
        count: 1,
      });
      return true;
    }

    // Check limit
    if (existing.count >= maxPerMinute) {
      return false;
    }

    // Increment
    await ctx.db.patch(existing._id, {
      count: existing.count + 1,
    });
    return true;
  },
});
```

---

## Step 4: Telegram API Helpers

### `convex/lib/telegramApi.ts`

```typescript
// Helper functions for calling the Telegram Bot API

const BASE_URL = "https://api.telegram.org/bot";

export async function sendMessage(
  token: string,
  chatId: number,
  text: string,
  options?: {
    replyToMessageId?: number;
    parseMode?: "HTML" | "Markdown" | "MarkdownV2";
  },
): Promise<any> {
  const url = `${BASE_URL}${token}/sendMessage`;

  const body: Record<string, any> = {
    chat_id: chatId,
    text: text,
  };

  if (options?.replyToMessageId) {
    body.reply_parameters = {
      message_id: options.replyToMessageId,
    };
  }

  if (options?.parseMode) {
    body.parse_mode = options.parseMode;
  }

  const response = await fetch(url, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(body),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Telegram API error: ${response.status} - ${error}`);
  }

  return response.json();
}

export async function sendChatAction(
  token: string,
  chatId: number,
  action: "typing" | "upload_document" = "typing",
): Promise<void> {
  const url = `${BASE_URL}${token}/sendChatAction`;

  await fetch(url, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      chat_id: chatId,
      action,
    }),
  });
}

// Call this once during setup to register your webhook
export async function setWebhook(
  token: string,
  webhookUrl: string,
  secret: string,
): Promise<any> {
  const url = `${BASE_URL}${token}/setWebhook`;

  const response = await fetch(url, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      url: webhookUrl,
      secret_token: secret,
      allowed_updates: ["message"],
      max_connections: 40,
    }),
  });

  return response.json();
}
```

---

## Step 5: AI Provider Abstraction

### `convex/lib/ai.ts`

```typescript
// Unified interface for Claude and OpenAI

export interface ConversationMessage {
  role: "user" | "assistant";
  content: string;
}

export interface AIResponse {
  text: string;
  inputTokens?: number;
  outputTokens?: number;
}

// â”€â”€ Claude (Anthropic) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function callClaude(
  apiKey: string,
  model: string,
  systemPrompt: string,
  messages: ConversationMessage[],
): Promise<AIResponse> {
  const response = await fetch("https://api.anthropic.com/v1/messages", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      "x-api-key": apiKey,
      "anthropic-version": "2023-06-01",
    },
    body: JSON.stringify({
      model,
      max_tokens: 1024,
      system: systemPrompt,
      messages: messages.map((m) => ({
        role: m.role,
        content: m.content,
      })),
    }),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Claude API error: ${response.status} - ${error}`);
  }

  const data = await response.json();
  return {
    text: data.content[0].text,
    inputTokens: data.usage?.input_tokens,
    outputTokens: data.usage?.output_tokens,
  };
}

// â”€â”€ OpenAI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function callOpenAI(
  apiKey: string,
  model: string,
  systemPrompt: string,
  messages: ConversationMessage[],
): Promise<AIResponse> {
  const response = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model,
      max_tokens: 1024,
      messages: [
        { role: "system", content: systemPrompt },
        ...messages.map((m) => ({
          role: m.role,
          content: m.content,
        })),
      ],
    }),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`OpenAI API error: ${response.status} - ${error}`);
  }

  const data = await response.json();
  return {
    text: data.choices[0].message.content,
    inputTokens: data.usage?.prompt_tokens,
    outputTokens: data.usage?.completion_tokens,
  };
}

// â”€â”€ Unified entry point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export async function generateResponse(
  provider: string,
  apiKey: string,
  model: string,
  systemPrompt: string,
  messages: ConversationMessage[],
): Promise<AIResponse> {
  switch (provider) {
    case "claude":
      return callClaude(apiKey, model, systemPrompt, messages);
    case "openai":
      return callOpenAI(apiKey, model, systemPrompt, messages);
    default:
      throw new Error(`Unknown AI provider: ${provider}`);
  }
}
```

---

## Step 6: Core Bot Logic

### `convex/telegram.ts`

```typescript
import { action, internalAction } from "./_generated/server";
import { internal } from "./_generated/api";
import { v } from "convex/values";
import { generateResponse, ConversationMessage } from "./lib/ai";
import { sendMessage, sendChatAction, setWebhook } from "./lib/telegramApi";

const DEFAULT_SYSTEM_PROMPT = `You are a helpful AI assistant in a Telegram group chat.
Keep responses concise and conversational â€” this is a chat, not an essay.
If multiple people are talking, pay attention to who said what.
Use plain text (no markdown) since Telegram groups render it poorly.
If you don't know something, say so.`;

// â”€â”€ Main message processor (scheduled by the webhook) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export const processMessage = internalAction({
  args: {
    chatId: v.number(),
    userId: v.number(),
    userName: v.string(),
    messageText: v.string(),
    messageId: v.number(),
  },
  handler: async (ctx, args) => {
    const token = process.env.TELEGRAM_BOT_TOKEN!;
    const aiProvider = process.env.AI_PROVIDER ?? "claude";
    const aiApiKey = process.env.AI_API_KEY!;
    const aiModel =
      process.env.AI_MODEL ??
      (aiProvider === "claude" ? "claude-sonnet-4-20250514" : "gpt-4o");

    try {
      // Show "typing..." indicator
      await sendChatAction(token, args.chatId);

      // Get chat config for custom system prompt
      const chatConfig = await ctx.runQuery(internal.messages.getChat, {
        chatId: args.chatId,
      });

      const systemPrompt = chatConfig?.systemPrompt ?? DEFAULT_SYSTEM_PROMPT;
      const maxContext = chatConfig?.maxContextMessages ?? 30;

      // Load recent conversation history
      const recentMessages = await ctx.runQuery(internal.messages.getRecent, {
        chatId: args.chatId,
        limit: maxContext,
      });

      // Build conversation for the AI
      const conversation: ConversationMessage[] = recentMessages.map((msg) => ({
        role: msg.role,
        content:
          msg.role === "user" ? `[${msg.userName ?? "Unknown"}]: ${msg.text}` : msg.text,
      }));

      // Call AI
      const aiResponse = await generateResponse(
        aiProvider,
        aiApiKey,
        aiModel,
        systemPrompt,
        conversation,
      );

      // Truncate if Telegram's 4096 char limit is hit
      let responseText = aiResponse.text;
      if (responseText.length > 4000) {
        responseText = responseText.slice(0, 4000) + "\n\n[truncated]";
      }

      // Store assistant response
      await ctx.runMutation(internal.messages.store, {
        chatId: args.chatId,
        role: "assistant",
        text: responseText,
      });

      // Send reply to Telegram
      await sendMessage(token, args.chatId, responseText, {
        replyToMessageId: args.messageId,
      });
    } catch (error: any) {
      console.error("Error processing message:", error);

      // Send error message to the chat
      await sendMessage(
        token,
        args.chatId,
        "Sorry, I encountered an error processing that message. Please try again.",
        { replyToMessageId: args.messageId },
      );
    }
  },
});

// â”€â”€ Webhook registration helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export const registerWebhook = action({
  args: {},
  handler: async () => {
    const token = process.env.TELEGRAM_BOT_TOKEN!;
    const secret = process.env.TELEGRAM_WEBHOOK_SECRET!;
    const convexUrl = process.env.CONVEX_SITE_URL!;
    const webhookUrl = `${convexUrl}/api/telegram-webhook`;

    const result = await setWebhook(token, webhookUrl, secret);
    console.log("Webhook registration result:", result);
    return result;
  },
});
```

**Note on `internal`:** The `processMessage` action and the mutations/queries it calls should be internal (not exposed to the public API). Mark queries/mutations as internal by exporting them from a file that uses `internalQuery`, `internalMutation`, etc. â€” or by referencing them through `internal.*` in the API. See Step 7 for how to make `messages.ts` exports internal.

### Making messages internal

Update `convex/messages.ts` to export internal versions:

```typescript
import { mutation, query, internalMutation, internalQuery } from "./_generated/server";

// Change the exports used by telegram.ts to internal:
export const store = internalMutation({
  /* ... same as before ... */
});
export const getRecent = internalQuery({
  /* ... same as before ... */
});
export const getChat = internalQuery({
  /* ... same as before ... */
});
export const checkRateLimit = internalMutation({
  /* ... same as before ... */
});

// Keep public versions only if you need them for a dashboard, etc.
```

---

## Step 7: HTTP Webhook Endpoint

### `convex/http.ts`

```typescript
import { httpRouter } from "convex/server";
import { httpAction } from "./_generated/server";
import { internal } from "./_generated/api";

const http = httpRouter();

http.route({
  path: "/api/telegram-webhook",
  method: "POST",
  handler: httpAction(async (ctx, request) => {
    // â”€â”€ 1. Validate the webhook secret â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const secret = process.env.TELEGRAM_WEBHOOK_SECRET;
    const headerSecret = request.headers.get("x-telegram-bot-api-secret-token");

    if (secret && headerSecret !== secret) {
      return new Response("Unauthorized", { status: 401 });
    }

    // â”€â”€ 2. Parse the Telegram update â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    let update: any;
    try {
      update = await request.json();
    } catch {
      return new Response("Bad request", { status: 400 });
    }

    // Only handle text messages (skip edits, media, etc.)
    const message = update.message;
    if (!message?.text) {
      return new Response("OK", { status: 200 });
    }

    const chatId: number = message.chat.id;
    const userId: number = message.from.id;
    const userName: string =
      message.from.first_name +
      (message.from.last_name ? ` ${message.from.last_name}` : "");
    const messageText: string = message.text;
    const messageId: number = message.message_id;
    const chatTitle: string | undefined = message.chat.title;
    const botUsername = process.env.BOT_USERNAME ?? "";

    // â”€â”€ 3. Determine if the bot should respond â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    //    In groups, only respond when:
    //    - The bot is mentioned (@botname)
    //    - The message is a reply to the bot's message
    //    - The message starts with a / command
    const isPrivateChat = message.chat.type === "private";
    const isMentioned = messageText.includes(`@${botUsername}`);
    const isReplyToBot = message.reply_to_message?.from?.username === botUsername;
    const isCommand = messageText.startsWith("/");

    const shouldRespond = isPrivateChat || isMentioned || isReplyToBot || isCommand;

    if (!shouldRespond) {
      // Still store the message for context, but don't respond
      await ctx.runMutation(internal.messages.store, {
        chatId,
        userId,
        userName,
        role: "user" as const,
        text: messageText,
        telegramMessageId: messageId,
      });
      return new Response("OK", { status: 200 });
    }

    // â”€â”€ 4. Handle commands â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if (isCommand) {
      const command = messageText.split(" ")[0].split("@")[0];

      if (command === "/start" || command === "/help") {
        const { sendMessage } = await import("./lib/telegramApi");
        await sendMessage(
          process.env.TELEGRAM_BOT_TOKEN!,
          chatId,
          "ðŸ‘‹ Hi! I'm an AI assistant. Mention me with @" +
            botUsername +
            " or reply to my messages to chat.\n\n" +
            "Commands:\n" +
            "/help â€” Show this message\n" +
            "/reset â€” Clear conversation history",
        );
        return new Response("OK", { status: 200 });
      }

      // Add more commands as needed (/reset, /setprompt, etc.)
    }

    // â”€â”€ 5. Rate limit check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const allowed = await ctx.runMutation(internal.messages.checkRateLimit, {
      chatId,
      userId,
      maxPerMinute: 10,
    });

    if (!allowed) {
      const { sendMessage } = await import("./lib/telegramApi");
      await sendMessage(
        process.env.TELEGRAM_BOT_TOKEN!,
        chatId,
        "Don't be rarted and spam, calm down! ðŸ¤¨",
        { replyToMessageId: messageId },
      );
      return new Response("OK", { status: 200 });
    }

    // â”€â”€ 6. Store user message â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // Strip the @mention from the text before storing
    const cleanText = messageText
      .replace(new RegExp(`@${botUsername}\\b`, "gi"), "")
      .trim();

    await ctx.runMutation(internal.messages.store, {
      chatId,
      userId,
      userName,
      role: "user" as const,
      text: cleanText,
      telegramMessageId: messageId,
    });

    // Ensure chat exists in our DB
    await ctx.runMutation(internal.messages.ensureChat, {
      chatId,
      chatTitle,
    });

    // â”€â”€ 7. Schedule AI processing (async) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    await ctx.scheduler.runAfter(0, internal.telegram.processMessage, {
      chatId,
      userId,
      userName,
      messageText: cleanText,
      messageId,
    });

    // Return immediately â€” Telegram is happy
    return new Response("OK", { status: 200 });
  }),
});

export default http;
```

**Why async?** By returning `200` immediately and scheduling the AI work, we avoid:

- Telegram thinking the webhook is dead and retrying
- Holding the HTTP action open during the 3-10 second AI call
- HTTP action timeout concerns under high load

---

## Step 8: Deployment & Webhook Registration

### Deploy to Convex

```bash
npx convex deploy
```

### Register the webhook with Telegram

After deploying, run the webhook registration action:

```bash
npx convex run telegram:registerWebhook
```

This tells Telegram to send all updates to your Convex HTTP endpoint. The output should show `{ ok: true }`.

### Configure the bot via BotFather

1. Open `@BotFather` on Telegram
2. `/setprivacy` â†’ Select your bot â†’ **Disable** (so the bot sees group messages for context)
3. `/setdescription` â†’ Set a description
4. `/setcommands` â†’ Set:
   ```
   help - Show help message
   reset - Clear conversation history
   ```

### Add the bot to a group

Add your bot to any Telegram group. Mention it with `@your_bot_username` to start chatting.

---

## Conversation Context Strategy

The bot stores all group messages (even ones it doesn't respond to) so it has full context. When building the AI prompt, it:

1. Fetches the last N messages (default 30) from that group
2. Prefixes user messages with `[FirstName]: message` so the AI knows who said what
3. Sends the full conversation to the AI with the system prompt

### Context window management

For busy groups, 30 messages might exceed the AI's ideal input size. You can tune this with more sophisticated approaches:

- **Token counting**: Estimate tokens (1 token â‰ˆ 4 chars) and cap at ~4,000 tokens of context
- **Summarization**: Periodically summarize older messages and store the summary
- **Relevance window**: Only include messages from the last 2 hours

Example token-budget approach for `getRecent`:

```typescript
export const getRecentWithBudget = internalQuery({
  args: {
    chatId: v.number(),
    maxTokens: v.optional(v.number()),
  },
  handler: async (ctx, args) => {
    const maxTokens = args.maxTokens ?? 4000;
    const messages = await ctx.db
      .query("messages")
      .withIndex("by_chat", (q) => q.eq("chatId", args.chatId))
      .order("desc")
      .take(100); // Fetch more than we need

    // Walk backwards, counting rough tokens
    let tokenCount = 0;
    const result = [];
    for (const msg of messages) {
      const estimatedTokens = Math.ceil(msg.text.length / 4);
      if (tokenCount + estimatedTokens > maxTokens) break;
      tokenCount += estimatedTokens;
      result.push(msg);
    }

    return result.reverse();
  },
});
```

---

## Error Handling & Edge Cases

### Telegram message length limit

Telegram caps messages at **4,096 characters**. The bot truncates long AI responses. For very long answers, you could split into multiple messages:

```typescript
function splitMessage(text: string, maxLen = 4000): string[] {
  const parts: string[] = [];
  while (text.length > 0) {
    if (text.length <= maxLen) {
      parts.push(text);
      break;
    }
    // Try to split at a newline
    let splitAt = text.lastIndexOf("\n", maxLen);
    if (splitAt === -1 || splitAt < maxLen * 0.5) {
      splitAt = maxLen;
    }
    parts.push(text.slice(0, splitAt));
    text = text.slice(splitAt).trimStart();
  }
  return parts;
}
```

### AI API failures

The `processMessage` action wraps the AI call in try/catch and sends an error message to the chat. For production, add:

- **Retry logic**: Retry the AI call once on 5xx errors or timeouts
- **Circuit breaker**: If the AI API fails repeatedly, stop trying for a few minutes
- **Logging**: Convex's built-in logging captures `console.error` output in the dashboard

### Duplicate updates

Telegram may rarely send the same update twice. You can deduplicate by storing `update_id` and checking before processing. For most bots this isn't necessary â€” the worst case is a duplicate AI response.

---

## Cost Estimation

### Convex costs (per message processed)

| Resource             | Usage per message          | Free tier included   | Overage cost |
| -------------------- | -------------------------- | -------------------- | ------------ |
| HTTP action call     | 1                          | 1M function calls/mo | $2.2/million |
| Mutation (store msg) | 2 (user + assistant)       | â†‘ counted together   | â†‘            |
| Query (get context)  | 1-2                        | â†‘                    | â†‘            |
| Action compute       | ~5s Ã— 0.5GB â‰ˆ 0.0007 GB-hr | 20 GB-hr/mo          | $0.33/GB-hr  |
| DB bandwidth         | ~2-5 KB                    | 1 GB/mo              | $0.22/GB     |

**Free tier capacity:** ~25,000-29,000 messages/month before paying Convex anything.

### AI API costs (per message)

| Provider  | Model                     | Approx cost per message |
| --------- | ------------------------- | ----------------------- |
| Anthropic | claude-sonnet-4-20250514  | ~$0.005-0.02            |
| Anthropic | claude-haiku-4-5-20251001 | ~$0.001-0.005           |
| OpenAI    | gpt-4o                    | ~$0.005-0.02            |
| OpenAI    | gpt-4o-mini               | ~$0.0005-0.002          |

**The AI API will be 10-100x more expensive than Convex.** For a budget bot, use Haiku or GPT-4o-mini.

---

## Optional Enhancements

### `/setprompt` command

Let group admins customize the bot's personality:

```typescript
// In the command handler section of http.ts:
if (command === "/setprompt") {
  const newPrompt = messageText.replace(/^\/setprompt(@\w+)?\s*/, "");
  if (!newPrompt) {
    await sendMessage(token, chatId, "Usage: /setprompt Your custom prompt here");
    return new Response("OK", { status: 200 });
  }
  // Update the chat's system prompt
  await ctx.runMutation(internal.messages.updateSystemPrompt, {
    chatId,
    systemPrompt: newPrompt,
  });
  await sendMessage(token, chatId, "System prompt updated âœ“");
  return new Response("OK", { status: 200 });
}
```

### `/reset` command

Clear conversation history:

```typescript
if (command === "/reset") {
  await ctx.runMutation(internal.messages.clearChat, { chatId });
  await sendMessage(token, chatId, "Conversation history cleared âœ“");
  return new Response("OK", { status: 200 });
}
```

### Multi-group dashboard

Since all data lives in Convex's database, you can build a simple React frontend using Convex's real-time queries to monitor bot usage, view conversations, and manage settings across all groups.

### Image support

Telegram sends photos as file IDs. You can download them via the Bot API, convert to base64, and send to Claude's vision API â€” all within a Convex action.

---

## Testing Locally

Convex doesn't run locally, but you can:

1. Use `npx convex dev` for hot-reloading during development
2. Test the webhook with curl against your dev deployment:

```bash
curl -X POST https://your-project.convex.site/api/telegram-webhook \
  -H "Content-Type: application/json" \
  -H "X-Telegram-Bot-Api-Secret-Token: your-secret" \
  -d '{
    "update_id": 1,
    "message": {
      "message_id": 1,
      "from": {"id": 123, "first_name": "Test"},
      "chat": {"id": -456, "type": "group", "title": "Test Group"},
      "text": "@your_bot hello!",
      "date": 1700000000
    }
  }'
```

3. Check the Convex dashboard logs for errors and function execution traces.

---

## Summary

This architecture gives you a production-ready Telegram AI bot with:

- **Zero infrastructure** to manage (Convex handles DB, compute, scaling)
- **Sub-second webhook response** (async processing pattern)
- **Full conversation context** (stored in Convex, queried per request)
- **Rate limiting** (per-user, per-group, transactional)
- **Swappable AI provider** (Claude or OpenAI, change an env var)
- **Per-group customization** (system prompts, context window size)
- **Low cost at scale** (AI tokens dominate, not serverless compute)
